defaults:
  - experiment: base_experiment
  - algorithm: mappo
  - task: vmas/football
  - model: layers/mlp
  - model@critic_model: layers/mlp
  - _self_

hydra:
  searchpath:
   # Tells hydra to add the default benchmarl configuration to its path
    - pkg://benchmarl/conf

seed: 0
n_frames_add_adversary: 30_000_000

task:
  max_steps: 500
  n_blue_agents: 2
  n_red_agents: 2
  disable_ai_red: True
  ball_mass: 0.25
  ai_start_vel: 1.5
  observe_teammates: True

experiment:

  sampling_device: "cuda"
  train_device: "cuda"

  share_policy_params: True
  prefer_continuous_actions: True

  gamma: 0.99
  lr: 0.00005
  clip_grad_norm: True
  clip_grad_val: 5

  soft_target_update: True
  polyak_tau: 0.005
  hard_target_update_frequency: 5

  exploration_eps_init: 0.8
  exploration_eps_end: 0.01
  exploration_anneal_frames: 1_000_000

  max_n_iters: null
  max_n_frames: 150_000_000

  on_policy_collected_frames_per_batch: 240_000
  on_policy_n_envs_per_worker: 480
  on_policy_n_minibatch_iters: 45
  on_policy_minibatch_size: 4096

  off_policy_collected_frames_per_batch: 6000
  off_policy_n_envs_per_worker: 12
  off_policy_n_optimizer_steps: 1000
  off_policy_train_batch_size: 128
  off_policy_memory_size: 5_000_000

  evaluation: True
  render: True
  evaluation_interval: 480_000
  evaluation_episodes: 200

  loggers: [wandb]
  create_json: True

  save_folder: null
  restore_file: null
  checkpoint_interval: 0
